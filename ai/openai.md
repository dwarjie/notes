# OpenAI API

I assume you're already familiar with OpenAI so I don't need to introduce it to you.

## Sample Response Code

Here is an example response code for text-generation using OpenAI in JavaScript.

```js
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
	model: "gpt-4.1",
	input: "Write a one-sentence bedtime story about a unicorn.",
});

console.log(response.output_text);
```

## Libraries

OpenAI has an SDK for different languages (e.g., JavaScript, Python, .NET, Java, and Go).

```bash
npm install openai
```

# Text Generation and Prompting

This markdown/notes is only specific for Text-generation since that's what I need for now.

You can use LLM's to generate text from a single prompt that can generate different responses like text, code, mathemarical equations, structure JSON, or like a human.

Here's another example of a prompt with the response:

```js
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
	model: "gpt-4.1",
	input: "Write a one-sentence bedtime story about a unicorn.",
});

console.log(response.output_text);
```

```bash
[
  {
    "id": "msg_67b73f697ba4819183a15cc17d011509",
    "type": "message",
    "role": "assistant",
    "content": [
      {
        "type": "output_text",
        "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
        "annotations": []
      }
    ]
  }
]
```

> NOTE: output array usually has more than one content item so you can't always assume that the output text is in the ouput_text[0]content[0].text. Official SDK's include an output_text property to get the response of the model for developer's convenience.

## Prompt Engineering

Prompt Engineering is the process of constructing an effective set of instruction for a model, such that it consistently generate an output that meet your requirements.

There are different techniques that work's with every model, like using message roles. But different model types (like reasoning models) might need to be prompted differently to produce a better result.

### Use a specific Model Snapshot

Even different snapshots of models within the same family could produce a different result.

That's why, it is highly recommended to stick to a specific model snapshot for your production (e.g., gpt-4.1-2025-04-14) for consistent behaviour.

### Message Roles and Instruction Following

You can provide instructions to your model when generating a response. The `instructions` parameter gives a high-level on how the model should behave (e.g., tone, goals, and examples of correct responses).

> The `instructions` parameter will take priority over a prompt in the `input` parameter.

```js
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
	model: "gpt-4.1",
	instructions: "Talk like a pirate.",
	input: "Are semicolons optional in JavaScript?",
});

console.log(response.output_text);
```

The code above is also the same when using an input message with `input` array.

```js
import OpenAI from "openai";
const client = new OpenAI();

const response = await client.responses.create({
	model: "gpt-4.1",
	input: [
		{
			role: "developer",
			content: "Talk like a pirate.",
		},
		{
			role: "user",
			content: "Are semicolons optional in JavaScript?",
		},
	],
});

console.log(response.output_text);
```

Here are the different roles you can use when creating your own prompt instruction and how OpenAI set a different level of priority to each other.

- Developer: developer messages are instruction provided by the application developer. Prioritized ahead of user messages.
- User: user messages are instruction provided by the end user. Prioritized behind developer messages.
- Assistant: Messages generated by the model have the assistant role.

### Message formatting with Markdown and XML

When creating your `developer` and `user` messages, you can use Markdown and XML language to provide hierarchy to your instructions.

1. You may use sections as headers:

```md
# Heading 1

## Heading 2

### Heading 3
```

2. You may user list for your contents:

```md
- Unordered List
- Unordered List
- Unordered List

  or

1. Ordered List
2. Ordered List
3. Ordered List
```

3. You may use XML Tags

```xml
<prompt>
  <task>Generate Python code</task>
  <description>Create a function that calculates the Fibonacci sequence up to a given number</description>
  <requirements>
    <item>Use recursion</item>
    <item>Include type hints</item>
    <item>Add docstring with examples</item>
  </requirements>
  <output_format>
    <language>python</language>
    <version>3.9+</version>
  </output_format>
</prompt>
```

> When using XML Tags, you should use intuitive tag names that clearly describe their content (e.g., <instructions>, <context>, <examples>, etc.)

#### Prompt Structure

In general, when generating your developer message, you should follow the following structure:

1. Identity: Describe the purpose, communication style, and high level goals of the model
2. Instructions: Provide a guide to the model such as:

- What rules should it follow?
- What should the model do, and never do?
  This sections could contain many subsections depending on your need.

3. Examples: Provide possible inputs along with the desired output from the model
4. Context: Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant.

Here's an example of a prompt with the use of XML and Markdown:

```md
# Identity

You are coding assistant that helps enforce the use of snake case
variables in JavaScript code, and writing code that will run in
Internet Explorer version 6.

# Instructions

- When defining variables, use snake case names (e.g. my_variable)
  instead of camel case names (e.g. myVariable).
- To support old browsers, declare variables using the older
  "var" keyword.
- Do not give responses with Markdown formatting, just return
  the code as requested.

# Examples

<user_query>
How do I declare a string variable for a first name?
</user_query>

<assistant_response>
var first_name = "Anna";
</assistant_response>
```

#### Prompt Caching

To save cost and faster processing, OpenAI provides prompt caching by default for all new models from gpt-4o and newer.

In order for the prompt to be Cache hit, place the static contents like instructions and examples at the beginning of your prompt and put all the user specific informations at the end. The reason for this is that the caching will will typically use the 256 tokens (depending on the model) to check if the initial prefix is cached or not.

Requirements:

- Only available for prompts with 1024 tokens or more, with cache hits occuring in increments of 128 tokens. Therefore, the number of cached tokens in a request will always fall within the following sequence: 1024, 1152, 1280, 1408, and so on, depending on the prompt's length.
- Can cache messages, images, tools, and structured outputs.

To be able to see if the prompt is cached, if the prompt is over 1024 tokens, the response object will display a `cached_tokens` field of the `usage.prompt_tokens_details` indicating how many prompt tokens were cache hit.

```json
"usage": {
  "prompt_tokens": 2006,
  "completion_tokens": 300,
  "total_tokens": 2306,
  "prompt_tokens_details": {
    "cached_tokens": 1920
  },
  "completion_tokens_details": {
    "reasoning_tokens": 0,
    "accepted_prediction_tokens": 0,
    "rejected_prediction_tokens": 0
  }
}
```
